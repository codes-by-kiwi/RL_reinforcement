{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBVxnSPJ4dNy",
        "outputId": "f790737f-83ab-4bda-a923-c462edf6be50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,Input\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cartpole_env = gym.make('CartPole-v1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ciTV_LuWY_F9"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import io\n",
        "sys.stdout = io.StringIO()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pkXhlmZG4e0R"
      },
      "outputs": [],
      "source": [
        "def build_neural_model_Q(state_space_size, action_space_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        Input(shape=(state_space_size,)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(24, activation='relu'),\n",
        "        layers.Dense(action_space_size, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TZjR4FT-4f9a"
      },
      "outputs": [],
      "source": [
        "discount_factor = 0.99\n",
        "explore_epsilon = 1.0\n",
        "epsilon_decay = 0.995\n",
        "maximum_epsilon = 0.75\n",
        "learning_value = 0.09\n",
        "num_episodes = 100\n",
        "num_repetitons = 5\n",
        "target_steps = 2000 ##across all episodes in a run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2UVA740B4hOv"
      },
      "outputs": [],
      "source": [
        "state_size_sample = cartpole_env.observation_space.shape[0]\n",
        "action_size_sample = cartpole_env.action_space.n\n",
        "model_sample = build_neural_model_Q(state_size_sample, action_size_sample)\n",
        "target_model_sample = build_neural_model_Q(state_size_sample, action_size_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4xTL4eK4iq-",
        "outputId": "cdf16cea-02d5-4148-cb3d-d24a3ba09085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated target network weights.\n"
          ]
        }
      ],
      "source": [
        "def update_target_network():\n",
        "    target_model_sample.set_weights(model_sample.get_weights())\n",
        "    print(\"Updated target network weights.\")\n",
        "\n",
        "update_target_network()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ROtzKhyA4j21"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_action(state):\n",
        "    if isinstance(state, tuple):\n",
        "        state = state[0]\n",
        "    state_reshaped = np.array(state).reshape(1, -1)\n",
        "\n",
        "    if np.random.uniform(0, 1) > explore_epsilon:\n",
        "        return cartpole_env.action_space.sample()\n",
        "    else: #choose best action according to knowledge\n",
        "        state_reshaped = state.reshape(1, -1)  # shape: (1, 4) # implicit batch definition of size 1\n",
        "        model_q_values = model_sample.predict(state_reshaped, verbose=0)\n",
        "        return np.argmax(model_q_values[0])  # according to the model the system would benefit from taking right action here. only different to left action by a slight amount\n",
        "\n",
        "# state = cartpole_env.reset()\n",
        "# print(f\"State: {state}\") # here there is a 1*4 output for state\n",
        "# action = epsilon_greedy_action(state)\n",
        "# print(f\"Chosen action: {action}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R2iqnmaK4lQP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "model_sample.compile(optimizer=Adam(learning_rate=0.003), loss='mean_squared_error')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY4g2POR4mWE",
        "outputId": "ebb66137-65ee-47fa-e725-ef6bf5c569e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Designated Priority: 2\n",
            "Q-values before update: [[-0.03099311 -0.02554449]]\n",
            "Q-values after update: [[-0.03512833  0.00294039]]\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#used to update the Q-values in the training stage\n",
        "def bellman_update_training(current_state, action, immediate_reward, next_state, terminated, steps_taken=None, max_steps=None):\n",
        "\n",
        "    if isinstance(current_state, tuple):\n",
        "        current_state = current_state[0]\n",
        "\n",
        "    cumulative_reward = immediate_reward # reward taken for taking action in current_state\n",
        "    priority = 1 #default priority for normal episode\n",
        "\n",
        "    #-----prioritycase2-----, give priority to bad experiences where going to the target takes alot of steps\n",
        "    # ----prioritycase3-----, give priority to bad experiences where we never reach the target but steps complete\n",
        "    #-----prioritycase4----, give priority to bad experiences where we reach sub-optimal solution but program thinks its target\n",
        "\n",
        "    #prioritycase2\n",
        "    if steps_taken is not None and max_steps is not None:\n",
        "        if steps_taken > 0.5 * max_steps:\n",
        "            priority = 2  # defining edge case\n",
        "\n",
        "    #prioritycase3\n",
        "    if terminated and steps_taken < max_steps * 0.7:\n",
        "        priority = 3\n",
        "\n",
        "    #prioritycase4\n",
        "    if not terminated and immediate_reward<1:\n",
        "        priority = 4\n",
        "\n",
        "    print(f\"Designated Priority: {priority}\")\n",
        "\n",
        "    # updating target if epsiode is not terminated\n",
        "    if not terminated:\n",
        "        next_state_reshaped = next_state.reshape(1, -1)\n",
        "        next_q_values = target_model_sample.predict(next_state_reshaped,verbose=0)\n",
        "        cumulative_reward = immediate_reward + discount_factor * np.max(next_q_values[0]) # target is updated to be this\n",
        "\n",
        "    state_reshaped = current_state.reshape(1, -1)\n",
        "    q_values = model_sample.predict(state_reshaped, verbose=0) # expected future rewards from current state\n",
        "    print(f\"Q-values before update: {q_values}\")\n",
        "\n",
        "    if priority == 1:\n",
        "       learning_value = 0.1\n",
        "\n",
        "    elif priority == 2:\n",
        "        learning_value = 0.2\n",
        "\n",
        "    elif priority == 3:\n",
        "        learning_value = 0.5\n",
        "\n",
        "    elif priority == 4:\n",
        "        learning_value = 0.8\n",
        "\n",
        "    q_values[0][action] = (1 - learning_value) * q_values[0][action] + learning_value * cumulative_reward\n",
        "\n",
        "    model_sample.fit(state_reshaped, q_values, verbose=0) #train model on updated q value\n",
        "\n",
        "    q_values_updated = model_sample.predict(state_reshaped, verbose=0)\n",
        "    print(f\"Q-values after update: {q_values_updated}\")\n",
        "\n",
        "\n",
        "current_state = np.array([0.2, 0.1, 0.0, 0.0])\n",
        "next_state = np.array([0.3, 0.1, 0.0, 0.0])\n",
        "steps_taken = 10\n",
        "max_steps = 15\n",
        "immediate_reward = 1\n",
        "terminated = False\n",
        "priority = bellman_update_training(current_state, 1, immediate_reward, next_state, terminated, steps_taken=steps_taken, max_steps=max_steps)\n",
        "print('-' * 100)\n",
        "\n",
        "#we can test with different parameters to this function to see how priority changes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YwhW5HsX4nif"
      },
      "outputs": [],
      "source": [
        "#this function is used to train the neural network overall.\n",
        "# used to show program how to update the states, take actions and reset.\n",
        "# tracking the performance so it can be plotted using graphs\n",
        "\n",
        "\n",
        "\n",
        "def overall_training_neural_network():\n",
        "    global explore_epsilon\n",
        "    rewards_per_episode = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = cartpole_env.reset()  # Reset environment to get initial state for each episode beginning\n",
        "        cumulative_reward = 0 # keeps track of cumulative reward accumulated in an episode\n",
        "        steps_taken = 0 # keeps track of number of steps taken in an episode\n",
        "        terminated = False #indicates program whether an episode or run has ended\n",
        "        truncated = False\n",
        "\n",
        "        while not terminated:\n",
        "            action = epsilon_greedy_action(state)  # Select action using epsilon-greedy function we defined\n",
        "            next_state, immediate_reward, terminated, info = cartpole_env.step(action)  # Take action in the environment\n",
        "            bellman_update_training(state, action, immediate_reward, next_state, terminated, steps_taken=steps_taken, max_steps=200)  # Update Q-values using bellman function defined\n",
        "            state = next_state\n",
        "            cumulative_reward += immediate_reward\n",
        "            steps_taken += 1\n",
        "\n",
        "        # Decay epsilon (exploration rate) after each episode\n",
        "        explore_epsilon = max(maximum_epsilon, explore_epsilon * epsilon_decay)\n",
        "\n",
        "        rewards_per_episode.append(cumulative_reward)\n",
        "\n",
        "        if episode % 10 == 0:  # Update target network periodically\n",
        "            update_target_network()\n",
        "\n",
        "        print(f\"Episode {episode + 1} out of total {num_episodes}, \"f\"Total Reward: {cumulative_reward}, Epsilon: {explore_epsilon:.2f}, \"\n",
        "        f\"Current State: {state}, Steps Taken: {steps_taken}\")\n",
        "\n",
        "\n",
        "    return rewards_per_episode\n",
        "\n",
        "# k = overall_training_neural_network()\n",
        "# print(k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co8y_2yLQ2up"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f3UdnWP4rkT"
      },
      "outputs": [],
      "source": [
        "rewards_all_runs = []\n",
        "\n",
        "for repetition in range(num_repetitons):\n",
        "    rewards_per_episode = overall_training_neural_network()\n",
        "    rewards_all_runs.append(rewards_per_episode)\n",
        "\n",
        "rewards_all_runs = np.array(rewards_all_runs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKGrKY8j4t1D"
      },
      "outputs": [],
      "source": [
        "for repetition in range(num_repetitons):\n",
        "    plt.plot(rewards_all_runs[repetition], label=f'Run {repetition + 1}')\n",
        "\n",
        "avg_rewards = np.mean(rewards_all_runs, axis=0)\n",
        "plt.plot(avg_rewards, label='Average Reward', color='black', linestyle='--', linewidth=2)\n",
        "\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Total Reward per Episode Across Multiple Runs with Averaging(No Smoothing and no Convolution)')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
